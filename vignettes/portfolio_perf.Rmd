---
title: "simaerep Portfolio Performance"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    code_folding: show
    collapse: false
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

# Load
```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(furrr))
suppressPackageStartupMessages(library(future))
suppressPackageStartupMessages(library(simaerep))
```

# Introduction

## Portfolio Configuration

The portfolio configuration consists of the following: 

**site parameters:**

- mean of all maximum patient visits
- sd of of all maximum patient visits
- total number patients

**study parameters:**

- mean AE per visit

We can use those parameters to simulate test data for assessing simaerep performance on a given portfolio.

We can start with a maximum aggregation of visit and n_ae on patient level starting with df_visit as we would use it for `simaerep::site_aggr()`. We can use `simaerep::get_config` to generate a valid portfolio configuration, which will automatically apply a few filters:

- remove patients with 0 visits
- minimum number of patients per study
- minimum number of sites per study
- anonymize study and site IDs

```{r}
df_visit1 <- sim_test_data_study(n_pat = 100, n_sites = 10,
   frac_site_with_ur = 0.4, ur_rate = 0.6)

df_visit1$study_id <- "A"

df_visit2 <- sim_test_data_study(n_pat = 100, n_sites = 10,
                                      frac_site_with_ur = 0.2, ur_rate = 0.1)

df_visit2$study_id <- "B"

df_visit <- bind_rows(df_visit1, df_visit2)

df_site_max <- df_visit %>%
  group_by(study_id, site_number, patnum) %>%
  summarise(max_visit = max(visit),
            max_ae = max(n_ae),
            .groups = "drop")

df_config <- simaerep::get_config(df_site_max, anonymize = TRUE, min_pat_per_study = 100, min_sites_per_study = 10)

df_config
```

## Simulate Portfolio from Configration

We can now apply sim_test_data_portfolio which uses `sim_test_data_study()` to generate artificial data on visit level.

```{r}
df_portf <- sim_test_data_portfolio(df_config)
df_portf
```


# Load Realisitc Configuration

The information contained in a portfolio configuration is very scarce and thus can be shared more easily within the industry. 

## Load

Here we load a realistic portfolio configuration.

```{r}
df_config <- readr::read_csv("vignettes/ae_profile.csv")
```

## Simulate Portfolio

And again simulate artificial visit level data. Using parallel processing.

```{r}
plan(multisession, workers = 18)

df_portf <- sim_test_data_portfolio(df_config, parallel = TRUE, progress = TRUE)
df_portf
```

## Get Under-Reporting Probability for Different Under Reporting Scenarios

The performance of detecting AE under-reporting is dependent on three things:

- the higher the mean AE per visit on study level the better
- the higher the number of patients at an under-reporting site the better
- the higher the number of under-reporting sites the worse

In our initial usability assessment we have fixed those parameters. Here we are going leave them as they are in the portfolio. The vanilla version of our artificial portfolio data does not contain any under-reporting sites yet. However `simaerep::sim_ur_scenarios()` will apply under-reporting scenarios to each site. Reducing the number of AEs by a given under-reporting (ur_rate) for all patients at the site and add the corresponding under-reporting statistics. Since the under-reporting probability is also affected by the number of other sites that are under-reporting we additionally calculate under-reporting statistics in a scenario where additional under reporting sites are present. For this we use the median number of patients per site at the study to calculate the final number of patients for which we lower the AEs in a given under-reporting scenario. 

```{r}

plan(multisession, workers = 18)

df_scen <- sim_ur_scenarios(df_portf,
                         extra_ur_sites = 5,
                         ur_rate = c(0.1, 0.25, 0.5, 0.75, 1),
                         parallel = TRUE,
                         poisson = TRUE,
                         prob_lower = TRUE,
                         progress = TRUE)

df_scen

readr::write_csv(df_scen, file = "vignettes/scen.csv")
```


## Portfolio Performance

We can calculate the portfolio performance as the overall true positive rate (tpr as tp/P) on the basis of desired false positive rates (fpr as fp/P).
We calculate a threshold based on the desired fpr using the vanilla scenario with no under-reporting sites. Then we check how many sites with known under-reporting get flagged to calculate tpr.

```{r}
get_portf_perf <- function(df_scen, stat = "prob_low_prob_ur", fpr = c(0.001, 0.01, 0.05)) {
  
  if (anyNA(df_scen[[stat]])) {
    mes <- df_scen %>%
      mutate(extra_ur_sites = as.factor(extra_ur_sites)) %>%
      group_by(extra_ur_sites, .drop = FALSE) %>%
      mutate(n_sites_total = n_distinct(site_number)) %>%
      group_by(extra_ur_sites, n_sites_total) %>%
      filter(is.na(.data[[stat]])) %>%
      summarise(n = n_distinct(site_number), .groups = "drop") %>%
      mutate(ratio_sites_with_na = n / ifelse(is.na(n_sites_total), 0, n_sites_total)) %>%
      select(extra_ur_sites, ratio_sites_with_na) %>%
      knitr::kable() %>%
      paste(collapse = "\n")
    
    warning(
      paste("Some Simulation Scenarios have returned NA stat values.\n", mes))
    
  }
  
  stat_at_0 <- df_scen %>%
    filter(ur_rate == 0, frac_pat_with_ur == 0) %>%
    pull(.data[[stat]])
  
  df_thresh <- tibble(
      fpr = fpr
    ) %>%
    mutate(thresh = map_dbl(fpr, ~ quantile(stat_at_0, probs =  1 - .)))
    
  
  df_prep <- df_scen %>%
    mutate(data = list(df_thresh)) %>%
    unnest(data) %>%
    mutate(stat = .data[[stat]]) %>%
    group_by(fpr, thresh, extra_ur_sites, ur_rate) %>%
    summarise(
      dr = sum(ifelse(.data$stat >= thresh, 1, 0), na.rm = TRUE) / 
           n_distinct(paste(study_id, site_number)),
      .groups = "drop")
    
  df_prep_0 <- df_prep %>%
    filter(ur_rate == 0) %>%
    mutate(extra_ur_sites = list(unique(df_prep$extra_ur_sites))) %>%
    unnest(extra_ur_sites)
  
  df_prep_gr0 <- df_prep %>%
    filter(ur_rate > 0)
  
  bind_rows(df_prep_0, df_prep_gr0) %>%
    arrange(fpr, ur_rate)
}

df_perf <- get_portf_perf(df_scen)

df_perf %>%
    pivot_wider(
    names_from = extra_ur_sites,
    values_from = dr,
    names_prefix = "extra_ur_sites_"
  ) %>%
  knitr::kable(digits = 3)

```


# Benchmark simaerep Using Portfolio Performance

One of the latest update to simaerep was an improvement to the visit_med75 calculation. We can check how this has affected portfolio performance.

```{r}
plan(multisession, workers = 18)

df_scen_old_visit_med75 <- sim_ur_scenarios(df_portf,
                                           extra_ur_sites = 5,
                                           ur_rate = c(0.1, 0.25, 0.5, 0.75, 1),
                                           parallel = TRUE,
                                           poisson = TRUE,
                                           prob_lower = TRUE,
                                           progress = TRUE,
                                           site_aggr_args = list(method = "med75")) # default is "med75_adj"

readr::write_csv(df_scen_old_visit_med75, file = "vignettes/scen_old.csv")

```

```{r}
# df_scen_old_visit_med75 <- readr::read_csv("vignettes/scen_old.csv")

df_perf_old <- get_portf_perf(df_scen_old_visit_med75)

df_perf_old %>%
    pivot_wider(
    names_from = extra_ur_sites,
    values_from = dr,
    names_prefix = "extra_ur_sites_"
  ) %>%
  knitr::kable(digits = 3)

```

```{r}
df_perf %>%
  mutate(type = "med75_adj") %>%
  bind_rows(
    df_perf_old %>%
      mutate(type = "med75")
  ) %>%
  ggplot(aes(x = fpr, y = dr, color = type)) +
    geom_line() +
    geom_point() +
    facet_grid(ur_rate ~ extra_ur_sites)
```

